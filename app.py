import gspread
import pandas as pd
import streamlit as st
from oauth2client.service_account import ServiceAccountCredentials
from langchain.docstore.document import Document
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.embeddings.base import Embeddings
from langchain.prompts import PromptTemplate
import google.generativeai as palm
from config import get_secret

GEMINI_API_KEY = get_secret("GEMINI_API_KEY")
CREDENCIAIS_JSON = get_secret("GOOGLE_CREDENTIALS_JSON")

# =========================
# CONFIGURA√á√ÉO GEMINI
# =========================
palm.configure(api_key=GEMINI_API_KEY)

# =========================
# 1. Conex√£o com Google Sheets
# =========================
def carregar_planilha(nome_planilha, aba, credenciais_json):
    scope = ["https://spreadsheets.google.com/feeds",
             "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_name(credenciais_json, scope)
    client = gspread.authorize(creds)
    sheet = client.open(nome_planilha).worksheet(aba)
    data = sheet.get_all_records()
    df = pd.DataFrame(data)

    return df


# =========================
# 2. Transformar em documentos (blocos de 50 linhas)
# =========================
def criar_documentos(df, chunk_size=50):
    documents = []
    for i in range(0, len(df), chunk_size):
        bloco = df.iloc[i:i+chunk_size]
        content = "\n".join(
            [", ".join([f"{k}: {v}" for k, v in row.items()]) for _, row in bloco.iterrows()]
        )
        doc = Document(page_content=content, metadata={"inicio": i, "source": "GoogleSheet"})
        documents.append(doc)
    return documents


# =========================
# 3. Wrapper customizado Gemini
# =========================
class GeminiEmbeddings(Embeddings):
    def embed_documents(self, texts):
        return [
            palm.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="RETRIEVAL_DOCUMENT"
            )["embedding"]
            for text in texts
        ]

    def embed_query(self, text):
        embedding = palm.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="RETRIEVAL_QUERY"
        )["embedding"]
        return embedding


# =========================
# 4. Criar vetor de embeddings (Chroma)
# =========================
def criar_vectorstore(documents):
    embeddings = GeminiEmbeddings()
    vectorstore = Chroma.from_documents(documents, embeddings)
    return vectorstore


# =========================
# 5. Criar agente RAG (PROMPT EM PORTUGU√äS)
# =========================
def criar_agente(vectorstore):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0.0, google_api_key=GEMINI_API_KEY)

    prompt_template = """Voc√™ √© um Analista de Dados S√™nior, especialista em transformar informa√ß√µes brutas em respostas claras, precisas e embasadas.
Seu objetivo √© analisar os dados fornecidos em <contexto> e responder √† pergunta em <pergunta> de forma objetiva, profissional e exclusivamente em portugu√™s.

<regras>
- Utilize apenas as informa√ß√µes presentes em `<contexto>`.
- Caso a resposta n√£o esteja dispon√≠vel, responda exatamente: **"Essa informa√ß√£o n√£o foi encontrada nos dados."**
- Mantenha a comunica√ß√£o em tom t√©cnico, mas simples e direto, sem jarg√µes desnecess√°rios.
- Quando relevante, apresente a resposta estruturada em t√≥picos, listas ou tabelas para maior clareza.
- Evite infer√™ncias que n√£o possam ser sustentadas pelos dados fornecidos.
- Se houver n√∫meros, estat√≠sticas ou m√©tricas, destaque-os de forma clara.
</regras>

Contexto:
{context}

Pergunta: {question}
Resposta:"""

    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    chain_type_kwargs = {"prompt": PROMPT}
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 30}),
        chain_type_kwargs=chain_type_kwargs
    )
    return qa


# =========================
# 6. Fun√ß√£o principal Streamlit
# =========================
def main():
    st.set_page_config(page_title="Agente de Dados AI", layout="wide")
    st.title("üìä Agente de Dados S√™nior (Gemini)")

    # ---- Carrega dados e configura agente (cache para n√£o refazer toda hora)
    @st.cache_resource(show_spinner="Carregando dados e criando embeddings...")
    def iniciar():
        df = carregar_planilha("base", "Dados", CREDENCIAIS_JSON)
        documents = criar_documentos(df)
        vectorstore = criar_vectorstore(documents)
        qa = criar_agente(vectorstore)
        return df, qa

    df, qa = iniciar()

    # ---- Sidebar de navega√ß√£o
    pagina = st.sidebar.radio("Escolha uma p√°gina", ["Chat com o Agente", "Visualizar Planilha"])

    if pagina == "Chat com o Agente":
        st.subheader("üí¨ Pergunte ao agente")
        pergunta = st.text_input("Digite sua pergunta:")

        if st.button("Enviar"):
            if pergunta:
                with st.spinner("O agente est√° processando..."):
                    resposta = qa.invoke({"query": pergunta})
                    st.markdown("### üß† Resposta do Agente:")
                    st.write(resposta['result'])
            else:
                st.warning("Digite uma pergunta antes de enviar.")

    elif pagina == "Visualizar Planilha":
        st.subheader("üìã Dados da Planilha")
        st.dataframe(df)
        st.markdown(f"**Total de linhas:** {len(df)}")


if __name__ == "__main__":
    main()
